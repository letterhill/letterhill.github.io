[
  {
    "objectID": "study.html",
    "href": "study.html",
    "title": "Study Notes",
    "section": "",
    "text": "All study notes are auto-listed below. Add a YAML header to each notebook (first Markdown cell) for nice titles, dates, and categories:\n---\ntitle: \"Week 1 — Linear Regression\"\ndate: 2025-08-14\ncategories: [ML, Study]\ndescription: \"From hypothesis to gradient descent\"\n---\n## Machine Learning Notes\n- [Regression](notebooks\\ML\\5.회귀-checkpoint.ipynb)\n\n\n\n\n\n\n\n\n\n\n\n데이터 셀렉션 및 필터링\n\n\n\n\n\n\n\n\n\n\n\n\n교차검증 수행\n\n\n\n\n\n\n\n\n\n\n\n\nModel Selection 소개\n\n\n\n\n\n\n\n\n\n\n\n\n숫자 데이터 활용\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost 개요\n\n\n\n\n\n\n\n\n\n\n\n\n앙상블 학습 개요\n\n\n\n\n\n\n\n\n\n\n\n\n랜덤 포레스트의 개요\n\n\n\n\n\n\n\n\n\n\n\n\n리프 중심 트리 분할 (Leaf Wise)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n결정 트리 과적합 (Overfitting)\n\n\n\n\n\n\n\n\n\n\n\n\n규제 선형 모델이란?\n\n\n\n\n\n\n\n\n\n\n\n\n회귀란 무엇인가\n\n\n\n\n\n\n\n\n\n\n\n\n5.5 회귀\n\n\n\nML\n\n\n\n회귀 5.1~5.5\n\n\n\nAug 15, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/ML/5.회귀.html",
    "href": "notebooks/ML/5.회귀.html",
    "title": "회귀란 무엇인가",
    "section": "",
    "text": "회귀의 핵심: 최적의 회귀 계수를 찾는 것.\n분류는 예측값이 이산형 클래스 값이고, 회귀는 연속형 숫자깂이라는 게 가장 큰 차이!\n선형 회귀가 가장 많이 사용됨.\n선형 회귀: 실제 값과 예측 값의 차이를 최소화하는 직선형 회귀선을 최적화하는 방식 -&gt; 규제 방법에 따라 다양한 유형으로 나뉜다.\n선형 회귀 모델의 종류: 일반 선형 회귀, 릿지, 라쏘, 엘라스틱넷, 로지스틱 회귀"
  },
  {
    "objectID": "notebooks/ML/5.회귀.html#다중공선성multi-collinearity-문제",
    "href": "notebooks/ML/5.회귀.html#다중공선성multi-collinearity-문제",
    "title": "회귀란 무엇인가",
    "section": "다중공선성(multi-collinearity) 문제",
    "text": "다중공선성(multi-collinearity) 문제\n: 피처 간의 상관관계가 너무 높아 분산이 매우 커져서 오류에 민감해지는 문제. * 상간관게가 높은 피처가 많은 경우 -&gt; 독립적인 중요한 피처만 남기고 제거하거나 규제를 적용"
  },
  {
    "objectID": "notebooks/ML/5.회귀.html#회귀-평가-지표",
    "href": "notebooks/ML/5.회귀.html#회귀-평가-지표",
    "title": "회귀란 무엇인가",
    "section": "회귀 평가 지표",
    "text": "회귀 평가 지표\n\n1) MAE\n\nMean Absolute Error\n실제 값과 예측값의 차를 절댓값으로 변환해 평균한 것\n\\(MAE = 1/n\\sum_{i=1}^{n} |Yi-Y'i|\\) ### 2) MSE\nMean Squared Error\n실제 값과 예측값 차를 제곱해 평균한 것\n\\(MSE = 1/n\\sum_{i=1}^{n} (Yi-Y'i)^2\\) ### 3) RMSE\nMSE에 루트를 씌운 것 ### 4) \\(R^2\\)\n분산 기반으로 예측 성능 평가\n1에 가까울수록 예측 정확도가 높다.\n\\(R^2 = (예측값 Variance)/(실제값 Variance)\\)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.datasets import load_diabetes\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# boston 데이터 세트 \ndiabetes = load_diabetes()\ndiabetesDF = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ndiabetesDF['sick'] = diabetes.target\nprint(\"데이터 크기\", diabetesDF.shape)\ndiabetesDF.head()\n\n데이터 크기 (442, 11)\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\nsick\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n\n\n\n\n\n\nprint(diabetesDF['sick'].describe())\n\ncount    442.000000\nmean     152.133484\nstd       77.093005\nmin       25.000000\n25%       87.000000\n50%      140.500000\n75%      211.500000\nmax      346.000000\nName: sick, dtype: float64\n\n\n\nfig, axs = plt.subplots(figsize=(16, 8), ncols=5, nrows=2)\nlm_features = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\nfor i, feature in enumerate(lm_features):\n    row = int(i/4)\n    col = i%4\n    sns.regplot(x=feature, y='sick', data=diabetesDF, ax=axs[row][col])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[23], line 6\n      4 row = int(i/4)\n      5 col = i%4\n----&gt; 6 sns.regplot(x=feature, y='sick', data=diabetesDF, ax=axs[row][col])\n\nIndexError: index 2 is out of bounds for axis 0 with size 2\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ny_target = diabetesDF['sick']\nX_data = diabetesDF.drop(['sick'], axis=1, inplace=False)\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.3, random_state=156)\n# 선형 회귀 OLS로 학습/예측/평가 수행\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_preds = lr.predict(X_test)\nmse = mean_squared_error(y_test, y_preds)\nrmse = np.sqrt(mse)\n\nprint('MSE: {0:.3f}, RMSE: {1:.3f}'.format(mse, rmse))\nprint('Variance score: {0:.3f}'.format(r2_score(y_test, y_preds)))\n\nMSE: 2993.705, RMSE: 54.715\nVariance score: 0.497\n\n\n\nprint('절편 값:', lr.intercept_)\nprint('회귀 계수값:', np.round(lr.coef_, 1))\n\n절편 값: 152.38617209733573\n회귀 계수값: [   39.1  -251.9   468.8   305.3 -1146.9   788.    177.2   117.4   937.9\n    53.7]\n\n\n\n# 회귀 계수를 큰 값 순으로 정렬하기 위해 Series로 생성. 인덱스 칼럼명에 유의\ncoeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns)\ncoeff.sort_values(ascending=False)\n\ns5      937.9\ns2      788.0\nbmi     468.8\nbp      305.3\ns3      177.2\ns4      117.4\ns6       53.7\nage      39.1\nsex    -251.9\ns1    -1146.9\ndtype: float64\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\ny_target ="
  },
  {
    "objectID": "notebooks/ML/5.6 규제 선형 모델-checkpoint.html",
    "href": "notebooks/ML/5.6 규제 선형 모델-checkpoint.html",
    "title": "규제 선형 모델이란?",
    "section": "",
    "text": "기본 선형 모델의 비용함수: RSS만을 고려 -&gt; only focus on 학습 데이터\nRSS와 coefficient 크기 제어의 균형!\n\\(비용 함수 목표 = Min(RSS(W) + alpha * \\| W \\|_2^2 )\\)\nalpha: 학습 데이터 적합 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터 ## Regularization (규제) : 비용 함수에 alpha 값으로 페널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식 ### L2 규제 : W의 제곱에 대해 페널티를 부여하는 방식 ex) Ridge 회귀 ### L1 규제 : W의 절댓값에 대해 페널티를 부여 ex) Lasso 회귀\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.datasets import load_diabetes\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# boston 데이터 세트 \ndiabetes = load_diabetes()\ndiabetesDF = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ndiabetesDF['sick'] = diabetes.target\nprint(\"데이터 크기\", diabetesDF.shape)\ndiabetesDF.head()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ny_target = diabetesDF['sick']\nX_data = diabetesDF.drop(['sick'], axis=1, inplace=False)\n\n데이터 크기 (442, 11)"
  },
  {
    "objectID": "notebooks/ML/5.6 규제 선형 모델-checkpoint.html#라쏘-회귀",
    "href": "notebooks/ML/5.6 규제 선형 모델-checkpoint.html#라쏘-회귀",
    "title": "규제 선형 모델이란?",
    "section": "라쏘 회귀",
    "text": "라쏘 회귀\n\nW의 절댓값에 페널티를 부여하는 L1 규제를 선형 회귀에 적용한 것.\nL1 규제: \\(alpha * \\| W \\|_1\\)\n\\(라쏘 회귀 비용함수의 목표: RSS(W) + alpha * \\| W \\|_1\\) 식을 최소화하는 W를 찾는 것.\n불필요한 회귀 계수를 0으로 만들어 제거 -&gt; 적절한 피처만 회귀에 포함시키는 피처 선택의 특성을 가짐!\n\n\nfrom sklearn.linear_model import Lasso, ElasticNet\n\n# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DF로 반환\ndef get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None,\n                       verbose=True, return_coeff=True):\n    coeff_df = pd.DataFrame()\n    if verbose: print('#####', model_name, '#####')\n    for param in params:\n        if model_name == 'Ridge': model = Ridge(alpha=param)\n        elif model_name == 'Lasso': model = Lasso(alpha=param)\n        elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)\n        neg_mse_scores = cross_val_score(model, X_data_n,\n                                        y_target_n, scoring='neg_mean_squared_error', cv=5)\n        avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores))\n        print('alpha {0}일 떼 5 폴드 세트의 평균 RMSE: {1:.3f}'.format(params, avg_rmse))\n        # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출\n        model.fit(X_data_n, y_target_n)\n        if return_coeff:\n            coeff = pd.Series(data=model.coef_, index=X_data_n.columns)\n            colname = 'alpha: '+str(param)\n            coeff_df[colname] = coeff\n    return coeff_df\n# end of get_linear_regre_eval\n\n\n# 라쏘에 사용될 alpha 파라미터의 값을 정의하고 get_linear_reg_eval() 함수 호출\nlasso_alphas = [0.07, 0.1, 0.5, 1, 3]\ncoeff_lasso_df = get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)\n\n##### Lasso #####\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 54.734\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 54.843\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 57.190\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 62.009\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 77.264\n\n\n\n# 반환된 coeff_lasso_df를 첫 번째 칼럼순으로 내림차순 정렬해 회귀계수 DF 출력\nsort_column = 'alpha: '+str(lasso_alphas[0])\ncoeff_lasso_df.sort_values(by=sort_column, ascending=False)\n\n\n\n\n\n\n\n\nalpha: 0.07\nalpha: 0.1\nalpha: 0.5\nalpha: 1\nalpha: 3\n\n\n\n\nbmi\n519.972636\n517.186795\n471.041874\n367.703860\n0.0\n\n\ns5\n500.793770\n483.912648\n408.022685\n307.605418\n0.0\n\n\nbp\n287.162043\n275.077235\n136.504084\n6.298858\n0.0\n\n\ns6\n45.225216\n33.673965\n0.000000\n0.000000\n0.0\n\n\nage\n-0.000000\n-0.000000\n0.000000\n0.000000\n0.0\n\n\ns2\n-0.000000\n-0.000000\n-0.000000\n0.000000\n0.0\n\n\ns4\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n\n\ns1\n-80.686661\n-52.539365\n-0.000000\n0.000000\n0.0\n\n\nsex\n-178.568523\n-155.359976\n-0.000000\n-0.000000\n0.0\n\n\ns3\n-217.692082\n-210.157991\n-58.319017\n-0.000000\n-0.0\n\n\n\n\n\n\n\n-&gt; alpha의 크기가 증가함에 따라 일부 피처의 회귀 계수는 아예 0으로 바뀐다.; 피처 선택의 효과!"
  },
  {
    "objectID": "notebooks/ML/5.6 규제 선형 모델-checkpoint.html#엘라스틱넷-회귀",
    "href": "notebooks/ML/5.6 규제 선형 모델-checkpoint.html#엘라스틱넷-회귀",
    "title": "규제 선형 모델이란?",
    "section": "엘라스틱넷 회귀",
    "text": "엘라스틱넷 회귀\n: L2 규제와 L1 규제를 결합한 회귀 * 엘라스틱넷 비용함수의 목표: \\(RSS(W) + alpha2 * \\| W \\|_2^2 + alpha1 * \\| W \\|_1)\\) 식을 최소화하는 W를 찾는 것 * alpha값에 따라 회귀 계수의 값이 급격하게 변동하는 라쏘 회귀의 단점을 극복하기 위해 L2 규제를 라쏘 회귀에 추가한 것 * 엘라스틱의 단점: 수행시간이 길다. * 사이킷런은 ElasticNet 클래스를 통해 구현 ## ElasticNet 클래스의 주요 파라미터 * alpha * l1_ratio\n\n# 엘라스틱넷에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출\n# l1_ratio는 0.7로 고정\nelastic_alphas = [0.07, 0.1, 0.5, 1, 3]\ncoeff_elastic_df = get_linear_reg_eval('ElasticNet', params=elastic_alphas,\n                                      X_data_n=X_data, y_target_n=y_target)\n\n##### ElasticNet #####\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 69.464\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 71.268\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 76.032\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 76.805\nalpha [0.07, 0.1, 0.5, 1, 3]일 떼 5 폴드 세트의 평균 RMSE: 77.262\n\n\n\n# 반환된 coeff_elastic_df를 첫 번째 칼럼순으로 내림차순 정렬해 회귀계수 DF 출력\nsort_column = 'alpha: '+str(elastic_alphas[0])\ncoeff_elastic_df.sort_values(by=sort_column, ascending=False)\n\n\n\n\n\n\n\n\nalpha: 0.07\nalpha: 0.1\nalpha: 0.5\nalpha: 1\nalpha: 3\n\n\n\n\nbmi\n78.372123\n57.938119\n11.544017\n4.744439\n0.053248\n\n\ns5\n72.720569\n54.278589\n10.997870\n4.487781\n0.000000\n\n\nbp\n56.534960\n42.039222\n8.079540\n2.991006\n0.000000\n\n\ns4\n49.239404\n37.750621\n7.694049\n2.837828\n0.000000\n\n\ns6\n44.919698\n34.082378\n6.608643\n2.266564\n0.000000\n\n\ns1\n19.039090\n15.192917\n2.527620\n0.205676\n0.000000\n\n\nage\n18.908138\n14.496770\n2.029207\n0.000000\n0.000000\n\n\ns2\n12.618259\n10.650735\n1.604738\n0.000000\n0.000000\n\n\nsex\n-0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\ns3\n-48.441153\n-36.339227\n-6.932461\n-2.417486\n-0.000000"
  },
  {
    "objectID": "notebooks/ML/4.4. 랜덤 포레스트.html",
    "href": "notebooks/ML/4.4. 랜덤 포레스트.html",
    "title": "랜덤 포레스트의 개요",
    "section": "",
    "text": "배깅의 대표적인 알고리즘\n개별 알고리즘은 결정 트리임. ## RandomForestClassifier 클래스\n사이킷런의 랜덤 포레스트 클래스\nn_estimator 파라미터로 subset 수 부여\ndef get_new_feature_name_df(old_feature_name_df):\n    feature_dup_df = pd.DataFrame(\n        data=old_feature_name_df.groupby('column_name').cumcount(),\n        columns=['dup_cnt']\n    ).reset_index()\n\n    new_feature_name_df = pd.merge(\n        old_feature_name_df.reset_index(), feature_dup_df, how='outer'\n    )\n\n    # iloc 사용해서 FutureWarning 방지\n    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(\n        lambda x: x.iloc[0] + '_' + str(x.iloc[1]) if x.iloc[1] &gt; 0 else x.iloc[0],\n        axis=1\n    )\n\n    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n    return new_feature_name_df\nimport pandas as pd\n\ndef get_human_dataset( ):\n    \n    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\n    feature_name_df = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\features.txt\",sep=r'\\s+',\n                        header=None,names=['column_index','column_name'])\n    \n    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성. \n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    \n    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n    \n    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용\n    X_train = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\train\\X_train.txt\",sep=r'\\s+', names=feature_name )\n    X_test = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\test\\X_test.txt\",sep=r'\\s+', names=feature_name)\n    \n    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여\n    y_train = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\train\\y_train.txt\",sep=r'\\s+',header=None,names=['action'])\n    y_test = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\test\\y_test.txt\",sep=r'\\s+',header=None,names=['action'])\n    \n    # 로드된 학습/테스트용 DataFrame을 모두 반환 \n    return X_train, X_test, y_train, y_test\n\n\nX_train, X_test, y_train, y_test = get_human_dataset()\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 결정 트리에서 사용한 함수를 이용해 학습/테스트용 DF 반환\nX_train, X_test, y_train, y_test = get_human_dataset()\n\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train)\npred = rf_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\nprint('랜포 정확도: {0:.4f}'.format(accuracy))\n\n랜포 정확도: 0.9237"
  },
  {
    "objectID": "notebooks/ML/4.4. 랜덤 포레스트.html#부스팅-알고리즘",
    "href": "notebooks/ML/4.4. 랜덤 포레스트.html#부스팅-알고리즘",
    "title": "랜덤 포레스트의 개요",
    "section": "부스팅 알고리즘",
    "text": "부스팅 알고리즘\n\nweak learner를 순차적 학습/예측하면서 잘못 에측한 데이터에 가중치를 부여하여 오류를 개선해나가며 학습하는 방식\nex) AdaBoost, 그래디언트 부스트\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nX_train, X_test, y_train, y_test = get_human_dataset()\n\n# GBM 수행 시간 측정을 위함. 시작 시간 설정.\nstart_time = time.time()\n\ngb_clf = GradientBoostingClassifier(random_state=0)\ngb_clf.fit(X_train, y_train)\ngb_pred = gb_clf.predict(y_test, gb_pred)\n\nprint('GBM 정확도: {0:.4f}'.format(gb_accuracy))\nprint('GBM 수행 시간: {0:.1f}초'.format(time.time()-start_time))"
  },
  {
    "objectID": "notebooks/ML/4.4. 랜덤 포레스트.html#xgboost-개요",
    "href": "notebooks/ML/4.4. 랜덤 포레스트.html#xgboost-개요",
    "title": "랜덤 포레스트의 개요",
    "section": "XGBoost 개요",
    "text": "XGBoost 개요\n\nGBM의 단점인 느린 수행 시간 및 과적합 규제 부재 등의 문제를 해결함.\n파이썬 패키지명: xgboost"
  },
  {
    "objectID": "notebooks/ML/4..6 XGBoost.html",
    "href": "notebooks/ML/4..6 XGBoost.html",
    "title": "XGBoost 개요",
    "section": "",
    "text": "GBM의 단점인 느린 수행 시간 및 과적합 규제 부재 등의 문제를 해결함.\n파이썬 패키지명: xgboost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier"
  },
  {
    "objectID": "notebooks/ML/4..6 XGBoost.html#early-stopping-조기-중단-기능",
    "href": "notebooks/ML/4..6 XGBoost.html#early-stopping-조기-중단-기능",
    "title": "XGBoost 개요",
    "section": "Early Stopping (조기 중단) 기능",
    "text": "Early Stopping (조기 중단) 기능\n: n_estimators에 도달하지 않더라도 예측 오류가 더 이상 개선되지 않으면 반복을 끝까지 수행X\n\nimport xgboost\n\nprint(xgboost.__version__)\n\n2.1.2"
  },
  {
    "objectID": "notebooks/ML/2장_사이킷런.html",
    "href": "notebooks/ML/2장_사이킷런.html",
    "title": "Model Selection 소개",
    "section": "",
    "text": "import sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n# 붓꽃 데이터 세트 로드\niris = load_iris()\n# iris.data는 iris 데이터 세트에서 피처만으로 이루어진 데이터를 numpy로 가짐\niris_data = iris.data\n# iris.target은 레이블 값을 numpy로 가짐\niris_label = iris.target\nprint('iris target값: ', iris_label)\nprint('iris target명: ', iris.target_names)\n# DF로 변환\niris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\niris_df.head(3)\n\niris target값:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\niris target명:  ['setosa' 'versicolor' 'virginica']\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\nX_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11)\n#Dtclf 객체 생성\ndt_clf = DecisionTreeClassifier(random_state=11)\n# 학습\ndt_clf.fit(X_train, y_train)\n\nDecisionTreeClassifier(random_state=11)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(random_state=11)\n# 예측 수행\npred = dt_clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint('정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))\n\n정확도: 0.9333\nfrom sklearn.datasets import load_iris\niris_data = load_iris()\nprint(type(iris_data))\n\n&lt;class 'sklearn.utils._bunch.Bunch'&gt;\nkeys = iris_data.keys()\nprint(keys)\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\nprint('\\n feature_names 의 type:',type(iris_data.feature_names))\nprint(' feature_names 의 shape:',len(iris_data.feature_names))\nprint(iris_data.feature_names)\n\nprint('\\n target_names 의 type:',type(iris_data.target_names))\nprint(' feature_names 의 shape:',len(iris_data.target_names))\nprint(iris_data.target_names)\n\nprint('\\n data 의 type:',type(iris_data.data))\nprint(' data 의 shape:',iris_data.data.shape)\nprint(iris_data['data'])\n\nprint('\\n target 의 type:',type(iris_data.target))\nprint(' target 의 shape:',iris_data.target.shape)\nprint(iris_data.target)\n\n\n feature_names 의 type: &lt;class 'list'&gt;\n feature_names 의 shape: 4\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n\n target_names 의 type: &lt;class 'numpy.ndarray'&gt;\n feature_names 의 shape: 3\n['setosa' 'versicolor' 'virginica']\n\n data 의 type: &lt;class 'numpy.ndarray'&gt;\n data 의 shape: (150, 4)\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]\n [5.4 3.9 1.7 0.4]\n [4.6 3.4 1.4 0.3]\n [5.  3.4 1.5 0.2]\n [4.4 2.9 1.4 0.2]\n [4.9 3.1 1.5 0.1]\n [5.4 3.7 1.5 0.2]\n [4.8 3.4 1.6 0.2]\n [4.8 3.  1.4 0.1]\n [4.3 3.  1.1 0.1]\n [5.8 4.  1.2 0.2]\n [5.7 4.4 1.5 0.4]\n [5.4 3.9 1.3 0.4]\n [5.1 3.5 1.4 0.3]\n [5.7 3.8 1.7 0.3]\n [5.1 3.8 1.5 0.3]\n [5.4 3.4 1.7 0.2]\n [5.1 3.7 1.5 0.4]\n [4.6 3.6 1.  0.2]\n [5.1 3.3 1.7 0.5]\n [4.8 3.4 1.9 0.2]\n [5.  3.  1.6 0.2]\n [5.  3.4 1.6 0.4]\n [5.2 3.5 1.5 0.2]\n [5.2 3.4 1.4 0.2]\n [4.7 3.2 1.6 0.2]\n [4.8 3.1 1.6 0.2]\n [5.4 3.4 1.5 0.4]\n [5.2 4.1 1.5 0.1]\n [5.5 4.2 1.4 0.2]\n [4.9 3.1 1.5 0.2]\n [5.  3.2 1.2 0.2]\n [5.5 3.5 1.3 0.2]\n [4.9 3.6 1.4 0.1]\n [4.4 3.  1.3 0.2]\n [5.1 3.4 1.5 0.2]\n [5.  3.5 1.3 0.3]\n [4.5 2.3 1.3 0.3]\n [4.4 3.2 1.3 0.2]\n [5.  3.5 1.6 0.6]\n [5.1 3.8 1.9 0.4]\n [4.8 3.  1.4 0.3]\n [5.1 3.8 1.6 0.2]\n [4.6 3.2 1.4 0.2]\n [5.3 3.7 1.5 0.2]\n [5.  3.3 1.4 0.2]\n [7.  3.2 4.7 1.4]\n [6.4 3.2 4.5 1.5]\n [6.9 3.1 4.9 1.5]\n [5.5 2.3 4.  1.3]\n [6.5 2.8 4.6 1.5]\n [5.7 2.8 4.5 1.3]\n [6.3 3.3 4.7 1.6]\n [4.9 2.4 3.3 1. ]\n [6.6 2.9 4.6 1.3]\n [5.2 2.7 3.9 1.4]\n [5.  2.  3.5 1. ]\n [5.9 3.  4.2 1.5]\n [6.  2.2 4.  1. ]\n [6.1 2.9 4.7 1.4]\n [5.6 2.9 3.6 1.3]\n [6.7 3.1 4.4 1.4]\n [5.6 3.  4.5 1.5]\n [5.8 2.7 4.1 1. ]\n [6.2 2.2 4.5 1.5]\n [5.6 2.5 3.9 1.1]\n [5.9 3.2 4.8 1.8]\n [6.1 2.8 4.  1.3]\n [6.3 2.5 4.9 1.5]\n [6.1 2.8 4.7 1.2]\n [6.4 2.9 4.3 1.3]\n [6.6 3.  4.4 1.4]\n [6.8 2.8 4.8 1.4]\n [6.7 3.  5.  1.7]\n [6.  2.9 4.5 1.5]\n [5.7 2.6 3.5 1. ]\n [5.5 2.4 3.8 1.1]\n [5.5 2.4 3.7 1. ]\n [5.8 2.7 3.9 1.2]\n [6.  2.7 5.1 1.6]\n [5.4 3.  4.5 1.5]\n [6.  3.4 4.5 1.6]\n [6.7 3.1 4.7 1.5]\n [6.3 2.3 4.4 1.3]\n [5.6 3.  4.1 1.3]\n [5.5 2.5 4.  1.3]\n [5.5 2.6 4.4 1.2]\n [6.1 3.  4.6 1.4]\n [5.8 2.6 4.  1.2]\n [5.  2.3 3.3 1. ]\n [5.6 2.7 4.2 1.3]\n [5.7 3.  4.2 1.2]\n [5.7 2.9 4.2 1.3]\n [6.2 2.9 4.3 1.3]\n [5.1 2.5 3.  1.1]\n [5.7 2.8 4.1 1.3]\n [6.3 3.3 6.  2.5]\n [5.8 2.7 5.1 1.9]\n [7.1 3.  5.9 2.1]\n [6.3 2.9 5.6 1.8]\n [6.5 3.  5.8 2.2]\n [7.6 3.  6.6 2.1]\n [4.9 2.5 4.5 1.7]\n [7.3 2.9 6.3 1.8]\n [6.7 2.5 5.8 1.8]\n [7.2 3.6 6.1 2.5]\n [6.5 3.2 5.1 2. ]\n [6.4 2.7 5.3 1.9]\n [6.8 3.  5.5 2.1]\n [5.7 2.5 5.  2. ]\n [5.8 2.8 5.1 2.4]\n [6.4 3.2 5.3 2.3]\n [6.5 3.  5.5 1.8]\n [7.7 3.8 6.7 2.2]\n [7.7 2.6 6.9 2.3]\n [6.  2.2 5.  1.5]\n [6.9 3.2 5.7 2.3]\n [5.6 2.8 4.9 2. ]\n [7.7 2.8 6.7 2. ]\n [6.3 2.7 4.9 1.8]\n [6.7 3.3 5.7 2.1]\n [7.2 3.2 6.  1.8]\n [6.2 2.8 4.8 1.8]\n [6.1 3.  4.9 1.8]\n [6.4 2.8 5.6 2.1]\n [7.2 3.  5.8 1.6]\n [7.4 2.8 6.1 1.9]\n [7.9 3.8 6.4 2. ]\n [6.4 2.8 5.6 2.2]\n [6.3 2.8 5.1 1.5]\n [6.1 2.6 5.6 1.4]\n [7.7 3.  6.1 2.3]\n [6.3 3.4 5.6 2.4]\n [6.4 3.1 5.5 1.8]\n [6.  3.  4.8 1.8]\n [6.9 3.1 5.4 2.1]\n [6.7 3.1 5.6 2.4]\n [6.9 3.1 5.1 2.3]\n [5.8 2.7 5.1 1.9]\n [6.8 3.2 5.9 2.3]\n [6.7 3.3 5.7 2.5]\n [6.7 3.  5.2 2.3]\n [6.3 2.5 5.  1.9]\n [6.5 3.  5.2 2. ]\n [6.2 3.4 5.4 2.3]\n [5.9 3.  5.1 1.8]]\n\n target 의 type: &lt;class 'numpy.ndarray'&gt;\n target 의 shape: (150,)\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]"
  },
  {
    "objectID": "notebooks/ML/2장_사이킷런.html#train_test_split-학습테스트-데이터셋-분리",
    "href": "notebooks/ML/2장_사이킷런.html#train_test_split-학습테스트-데이터셋-분리",
    "title": "Model Selection 소개",
    "section": "train_test_split() : 학습/테스트 데이터셋 분리",
    "text": "train_test_split() : 학습/테스트 데이터셋 분리\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\ndt_clf = DecisionTreeClassifier()\ntrain_data = iris.data\ntrain_label = iris.target\ndt_clf.fit(train_data, train_label)\n\n# 예측 수행\npred = dt_clf.predict(train_data)\nprint(\"정확도\", accuracy_score(train_label, pred))\n\n정확도 1.0\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\ndt_clf = DecisionTreeClassifier()\niris_data = load_iris()\n\nX_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, \n                                                    test_size=0.3, random_state=121)\n\n\ndt_clf.fit(X_train, y_train)\npred = dt_clf.predict(X_test)\nprint(\"정확도: {0:.4f}\".format(accuracy_score(y_test, pred)))\n\n정확도: 0.9556"
  },
  {
    "objectID": "notebooks/ML/2장_사이킷런.html#교차-검증",
    "href": "notebooks/ML/2장_사이킷런.html#교차-검증",
    "title": "Model Selection 소개",
    "section": "교차 검증",
    "text": "교차 검증\n\nK 폴드\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\niris = load_iris()\nfeatures = iris.data\nlabel= iris.target\ndt_clf = DecisionTreeClassifier()\n\n# KFold 폴드수는 5\nkfold = KFold(n_splits=5)\ncv_accuracy = []\nprint(\"데이터 세트 크기:\", features.shape[0])\n\n데이터 세트 크기: 150\n\n\n\nn_iter = 0\n\nfor train_index, test_index in kfold.split(features):\n    X_train, X_test = features[train_index], features[test_index]\n    y_train, y_test = label[train_index], label[test_index]\n    dt_clf.fit(X_train, y_train)\n    pred = dt_clf.predict(X_test)\n    n_iter += 1\n    # 정확도 측정\n    accuracy = np.round(accuracy_score(y_test, pred), 4)\n    train_size = X_train.shape[0]\n    test_size = X_test.shape[0]\n    print(\"\\n#{0} 교차 검증 정확도: {1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}\"\n          .format(n_iter, accuracy, train_size, test_size))\n    print(\"# {0} 검증 세트 인덱스: {1}\".format(n_iter, test_index))\n    cv_accuracy.append(accuracy)\n\nprint(\"평균 검증 정확도:\", np.mean(cv_accuracy))\n\n\n#1 교차 검증 정확도: 1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n# 1 검증 세트 인덱스: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29]\n\n#2 교차 검증 정확도: 0.9667, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n# 2 검증 세트 인덱스: [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n 54 55 56 57 58 59]\n\n#3 교차 검증 정확도: 0.8667, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n# 3 검증 세트 인덱스: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n 84 85 86 87 88 89]\n\n#4 교차 검증 정확도: 0.9333, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n# 4 검증 세트 인덱스: [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n 108 109 110 111 112 113 114 115 116 117 118 119]\n\n#5 교차 검증 정확도: 0.7333, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n# 5 검증 세트 인덱스: [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n 138 139 140 141 142 143 144 145 146 147 148 149]\n평균 검증 정확도: 0.9\n\n\n\nStratified K 폴드\n\n폴드마다 라벨별 데이터 분포도가 불균등한 것을 해결하기 위한 교차검증법\n전체 데이터의 라벨 분포도를 고려하여 폴드를 나눈다.\n\n\nimport pandas as pd\n\niris = load_iris()\n\niris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\niris_df['label']=iris.target\niris_df['label'].value_counts() # 라벨별로 1:1:1의 분포를 가짐을 확인할 수 있음.\n\nlabel\n0    50\n1    50\n2    50\nName: count, dtype: int64\n\n\n\nkfold = KFold(n_splits=3)\nn_iter=0\nfor train_index, test_index in kfold.split(iris_df):\n    n_iter += 1\n    label_train = iris_df['label'].iloc[train_index]\n    label_test = iris_df['label'].iloc[test_index]\n    print(\"교차 검증: {0}\".format(n_iter))\n    print(\"학습 레이블 데이터 분포:\", label_train.value_counts())\n    print(\"검증 레이블 데이터 분포:\", label_test.value_counts())\n\n교차 검증: 1\n학습 레이블 데이터 분포: label\n1    50\n2    50\nName: count, dtype: int64\n검증 레이블 데이터 분포: label\n0    50\nName: count, dtype: int64\n교차 검증: 2\n학습 레이블 데이터 분포: label\n0    50\n2    50\nName: count, dtype: int64\n검증 레이블 데이터 분포: label\n1    50\nName: count, dtype: int64\n교차 검증: 3\n학습 레이블 데이터 분포: label\n0    50\n1    50\nName: count, dtype: int64\n검증 레이블 데이터 분포: label\n2    50\nName: count, dtype: int64"
  },
  {
    "objectID": "notebooks/ML/2장_사이킷런.html#레이블-인코딩-label-encoding",
    "href": "notebooks/ML/2장_사이킷런.html#레이블-인코딩-label-encoding",
    "title": "Model Selection 소개",
    "section": "레이블 인코딩 (Label encoding)",
    "text": "레이블 인코딩 (Label encoding)\n\n라벨마다 숫자를 붙여서 인코딩하는 방식\n숫자의 크기를 중요도로 인식한다는 문제점이 있음\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nitems = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n\nencoder = LabelEncoder() # 객체 생성\nencoder.fit(items)\nlabels = encoder.transform(items)\nprint('인코딩 변환값:', labels)\n\n인코딩 변환값: [0 1 4 5 3 3 2 2]\n\n\n\n# encoder의 classes_ 피처는 0부터 순서대로 원본 라벨값을 가진다.\nprint(\"인코딩 클래스:\", encoder.classes_) \n\n인코딩 클래스: ['TV' '냉장고' '믹서' '선풍기' '전자레인지' '컴퓨터']\n\n\n\n# inverse_transform은 디코딩을 수행한다.\nprint(\"디코딩 원본 값:\", encoder.inverse_transform([4,5,2,0,1,1,3,3]))\n\n디코딩 원본 값: ['전자레인지' '컴퓨터' '믹서' 'TV' '냉장고' '냉장고' '선풍기' '선풍기']"
  },
  {
    "objectID": "notebooks/ML/2장_사이킷런.html#원-핫-인코딩one-hot-encoding",
    "href": "notebooks/ML/2장_사이킷런.html#원-핫-인코딩one-hot-encoding",
    "title": "Model Selection 소개",
    "section": "원-핫 인코딩(One-Hot encoding)",
    "text": "원-핫 인코딩(One-Hot encoding)\n\n라벨인코딩의 숫자 크기 문제를 해결하기 위한 인코딩 방식이다.\n숫자가 아닌 넘파이 형태로 특정 컬럼에만 1부여, 나머지는 0 이런 식으로 한다.\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\nitems = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n\n# 2차원 ndarray로 변환한다.\nitems = np.array(items).reshape(-1, 1)\n# 원핫 인코딩\noh_encoder = OneHotEncoder() # 객체 생성\noh_encoder.fit(items)\noh_labels = oh_encoder.transform(items)\n# 원핫 변환 결과는 희소행렬이라서 toarray()를 이용해 밀집 행렬로 변환\nprint(oh_labels.toarray())\nprint(oh_labels.shape)\n\n[[1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0. 0.]\n [0. 0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0.]]\n(8, 6)\n\n\n\nimport pandas as pd\ndf = pd.DataFrame({'item': ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']})\npd.get_dummies(df).astype(int)\n\n\n\n\n\n\n\n\nitem_TV\nitem_냉장고\nitem_믹서\nitem_선풍기\nitem_전자레인지\nitem_컴퓨터\n\n\n\n\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n0\n1\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n0\n0\n0\n0\n1\n\n\n4\n0\n0\n0\n1\n0\n0\n\n\n5\n0\n0\n0\n1\n0\n0\n\n\n6\n0\n0\n1\n0\n0\n0\n\n\n7\n0\n0\n1\n0\n0\n0"
  },
  {
    "objectID": "notebooks/ML/2장_사이킷런.html#피처-스케일링과-정규화",
    "href": "notebooks/ML/2장_사이킷런.html#피처-스케일링과-정규화",
    "title": "Model Selection 소개",
    "section": "피처 스케일링과 정규화",
    "text": "피처 스케일링과 정규화"
  },
  {
    "objectID": "notebooks/ML/2장_사이킷런.html#standardscaler",
    "href": "notebooks/ML/2장_사이킷런.html#standardscaler",
    "title": "Model Selection 소개",
    "section": "StandardScaler",
    "text": "StandardScaler\n\n표준화를 수행하는 함수\n(원본값-평균)/표준편차\n0~1 사이의 값을 반환\n\n\nfrom sklearn.datasets import load_iris\nimport pandas as pd\niris = load_iris()\niris_data = iris.data\niris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n\nprint(\"피처 평균값: \", iris_df.mean())\nprint(\"피처 분산값: \", iris_df.var())\n\n피처 평균값:  sepal length (cm)    5.843333\nsepal width (cm)     3.057333\npetal length (cm)    3.758000\npetal width (cm)     1.199333\ndtype: float64\n피처 분산값:  sepal length (cm)    0.685694\nsepal width (cm)     0.189979\npetal length (cm)    3.116278\npetal width (cm)     0.581006\ndtype: float64\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(iris_df)\niris_scaled = scaler.transform(iris_df)\n\niris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\nprint(\"피처 평균값 \", iris_df_scaled.mean())\nprint('피처 분산값 ', iris_df_scaled.var())\n\n피처 평균값  sepal length (cm)   -1.690315e-15\nsepal width (cm)    -1.842970e-15\npetal length (cm)   -1.698641e-15\npetal width (cm)    -1.409243e-15\ndtype: float64\n피처 분산값  sepal length (cm)    1.006711\nsepal width (cm)     1.006711\npetal length (cm)    1.006711\npetal width (cm)     1.006711\ndtype: float64\n\n\n\nMinMaxSclaer\n\n그냥 0~1 사이의 값으로 반환\n음수가 있을 때는 -1~1\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(iris_df)\niris_scaled = scaler.transform(iris_df)\n\niris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\nprint(\"피처 최솟값: \", iris_df_scaled.min())\nprint('파처 최대값: ', iris_df_scaled.max())\n# 결과가 0,1임을 확인할 수 있다\n\n피처 최솟값:  sepal length (cm)    0.0\nsepal width (cm)     0.0\npetal length (cm)    0.0\npetal width (cm)     0.0\ndtype: float64\n파처 최대값:  sepal length (cm)    1.0\nsepal width (cm)     1.0\npetal length (cm)    1.0\npetal width (cm)     1.0\ndtype: float64"
  },
  {
    "objectID": "notebooks/ML/1장 공부.html",
    "href": "notebooks/ML/1장 공부.html",
    "title": "데이터 셀렉션 및 필터링",
    "section": "",
    "text": "import pandas as pd\ntitanic_df = pd.read_csv(r'C:\\Users\\HOME\\Downloads\\Lab 3\\Lab 3\\titanic_train.csv')\ntitanic_df.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\nprint('DF 크기:', titanic_df.shape)\n\nDF 크기: (891, 12)\ntitanic_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\ntitanic_df.describe()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\nvalue_counts = titanic_df['Pclass'].value_counts()\nprint(value_counts)\n\nPclass\n3    491\n1    216\n2    184\nName: count, dtype: int64\ntitanic_pclass = titanic_df['Pclass']\nprint(type(titanic_pclass))\n\n&lt;class 'pandas.core.series.Series'&gt;\ntitanic_pclass.head()\n\n0    3\n1    1\n2    3\n3    1\n4    3\nName: Pclass, dtype: int64\nprint('titanic_df 데이터 건수', titanic_df.shape[0])\n\ntitanic_df 데이터 건수 891\nimport numpy as np\ncol_name1 = ['col1']\nlist1 = [1,2,3]\narray1 = np.array(list1)\nprint('array1 shape:', array1.shape)\n# 리스트를 이용해 DF 생성\ndf_list1 = pd.DataFrame(list1, columns=col_name1)\nprint(df_list1)\n# 넘파이 ndarray를 이용해 DF 생성\ndf_array1 = pd.DataFrame(array1, columns=col_name1)\nprint(df_array1)\n\narray1 shape: (3,)\n   col1\n0     1\n1     2\n2     3\n   col1\n0     1\n1     2\n2     3\n# 딕셔너리 -&gt; df 변환\n# key는 칼럼명, value는 리스트형으로 매핑\ndict = {'col1': [1,11], 'col2':[2,22], 'col3':[3,33]}\ndf_dict = pd.DataFrame(dict)\nprint(df_dict)\n\n   col1  col2  col3\n0     1     2     3\n1    11    22    33\n# df -&gt; ndarray 변환 : df의 .values 객체 이용\narray3 = df_dict.values\nprint(array3)\n\n[[ 1  2  3]\n [11 22 33]]\n# df -&gt; list 변환: values 객체를 tolist()\nlist3 = df_dict.values.tolist()\nprint(list3)\n\n[[1, 2, 3], [11, 22, 33]]\n# df -&gt; 딕셔너리 변환: to_dict()\ndict3 = df_dict.to_dict('list')\nprint(dict3)\n\n{'col1': [1, 11], 'col2': [2, 22], 'col3': [3, 33]}\ntitanic_df['Age_0']=0\ntitanic_df.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nAge_by+10\nAge_0\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n320.0\n0\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n480.0\n0\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n360.0\n0\ntitanic_df['Age_by_10']=titanic_df['Age']*10\ntitanic_df['Family_No']=titanic_df['SibSp']+titanic_df['Parch']+1\ntitanic_df.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nAge_by+10\nAge_0\nAge_by_10\nFamily_No\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n320.0\n0\n220.0\n2\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n480.0\n0\n380.0\n2\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n360.0\n0\n260.0\n1\ntitanic_df['Age_by_10']=titanic_df['Age_by_10']+100\ntitanic_df.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nAge_by+10\nAge_0\nAge_by_10\nFamily_No\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n320.0\n0\n320.0\n2\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n480.0\n0\n480.0\n2\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n360.0\n0\n360.0\n1\ntitanic_drop_df = titanic_df.drop('Age_0', axis=1)\ntitanic_drop_df.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nAge_by+10\nAge_by_10\nFamily_No\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n320.0\n320.0\n2\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n480.0\n480.0\n2\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n360.0\n360.0\n1\ntitanic_df.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nAge_by+10\nAge_0\nAge_by_10\nFamily_No\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n320.0\n0\n320.0\n2\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n480.0\n0\n480.0\n2\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n360.0\n0\n360.0\n1\n# inplace=True로 설정한 경우에는 원본 데이터 자체를 수정하며, 반환값이 없음.\ndrop_result = titanic_df.drop(['Age_0', 'Age_by_10', 'Family_No'], axis=1, inplace=True)\nprint('inplace=True로 drop 후 반환된 값:', drop_result)\ntitanic_df.head(3)\n\ninplace=True로 drop 후 반환된 값: None\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nAge_by+10\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n320.0\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n480.0\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n360.0\nprint(titanic_df.columns)\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Age_by+10'],\n      dtype='object')\npd.set_option('display.width', 1000)\npd.set_option('display.max_colwidth', 15)\nprint('#### before axis 0 drop ####')\nprint(titanic_df.head(3))\n\ntitanic_df.drop([0,1,2], axis=0, inplace=True)\n\nprint('#### after axis 0 drop ####')\nprint(titanic_df.head(3))\n\n#### before axis 0 drop ####\n   PassengerId  Survived  Pclass            Name     Sex   Age  SibSp  Parch  Ticket     Fare Cabin Embarked  Age_by+10\n3            4         1       1  Futrelle, M...  female  35.0      1      0  113803  53.1000  C123        S      450.0\n4            5         0       3  Allen, Mr. ...    male  35.0      0      0  373450   8.0500   NaN        S      450.0\n5            6         0       3  Moran, Mr. ...    male   NaN      0      0  330877   8.4583   NaN        Q        NaN\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[43], line 6\n      3 print('#### before axis 0 drop ####')\n      4 print(titanic_df.head(3))\n----&gt; 6 titanic_df.drop([0,1,2], axis=0, inplace=True)\n      8 print('#### after axis 0 drop ####')\n      9 print(titanic_df.head(3))\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581, in DataFrame.drop(self, labels, axis, index, columns, level, inplace, errors)\n   5433 def drop(\n   5434     self,\n   5435     labels: IndexLabel | None = None,\n   (...)\n   5442     errors: IgnoreRaise = \"raise\",\n   5443 ) -&gt; DataFrame | None:\n   5444     \"\"\"\n   5445     Drop specified labels from rows or columns.\n   5446 \n   (...)\n   5579             weight  1.0     0.8\n   5580     \"\"\"\n-&gt; 5581     return super().drop(\n   5582         labels=labels,\n   5583         axis=axis,\n   5584         index=index,\n   5585         columns=columns,\n   5586         level=level,\n   5587         inplace=inplace,\n   5588         errors=errors,\n   5589     )\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788, in NDFrame.drop(self, labels, axis, index, columns, level, inplace, errors)\n   4786 for axis, labels in axes.items():\n   4787     if labels is not None:\n-&gt; 4788         obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n   4790 if inplace:\n   4791     self._update_inplace(obj)\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830, in NDFrame._drop_axis(self, labels, axis, level, errors, only_slice)\n   4828         new_axis = axis.drop(labels, level=level, errors=errors)\n   4829     else:\n-&gt; 4830         new_axis = axis.drop(labels, errors=errors)\n   4831     indexer = axis.get_indexer(new_axis)\n   4833 # Case for non-unique axis\n   4834 else:\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070, in Index.drop(self, labels, errors)\n   7068 if mask.any():\n   7069     if errors != \"ignore\":\n-&gt; 7070         raise KeyError(f\"{labels[mask].tolist()} not found in axis\")\n   7071     indexer = indexer[~mask]\n   7072 return self.delete(indexer)\n\nKeyError: '[0, 1, 2] not found in axis'\n# Index 객체\n\n# 원본 파일 재로딩\ntitanic_df = pd.read_csv(r'C:\\Users\\HOME\\Downloads\\Lab 3\\Lab 3\\titanic_train.csv')\n# Index 객체 추출\nindexes = titanic_df.index\nprint(indexes)\n# Index 객체를 실제 값 array로 변환\nprint('Index 객체 array 값: \\n', indexes.values)\n\nRangeIndex(start=0, stop=891, step=1)\nIndex 객체 array 값: \n [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791\n 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809\n 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827\n 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845\n 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863\n 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881\n 882 883 884 885 886 887 888 889 890]\nprint(type(indexes.values))\nprint(indexes.values.shape)\nprint(indexes[:5].values)\nprint(indexes.values[:5])\nprint(indexes[6])\n\n&lt;class 'numpy.ndarray'&gt;\n(891,)\n[0 1 2 3 4]\n[0 1 2 3 4]\n6\nindexes[0]=5 #인덱스는 임의로 변경이 불가능하기 때문에 오류 발생!\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[51], line 1\n----&gt; 1 indexes[0]=5\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5371, in Index.__setitem__(self, key, value)\n   5369 @final\n   5370 def __setitem__(self, key, value) -&gt; None:\n-&gt; 5371     raise TypeError(\"Index does not support mutable operations\")\n\nTypeError: Index does not support mutable operations\nprint('단일 컬럼 데이터 추출:\\n', titanic_df['Pclass'].head(3))\nprint('\\n여러 컬럼들의 데이터 추출:\\n', titanic_df[['Survived', 'Pclass']].head(3))\nprint('[] 안에 있는 숫자 index는 KeyError 오류 발생',titanic_df[0]) # []인에는 칼럼명!\n\n단일 컬럼 데이터 추출:\n 0    3\n1    1\n2    3\nName: Pclass, dtype: int64\n\n여러 컬럼들의 데이터 추출:\n    Survived  Pclass\n0         0       3\n1         1       1\n2         1       3\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas\\\\_libs\\\\hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas\\\\_libs\\\\hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 0\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[55], line 3\n      1 print('단일 컬럼 데이터 추출:\\n', titanic_df['Pclass'].head(3))\n      2 print('\\n여러 컬럼들의 데이터 추출:\\n', titanic_df[['Survived', 'Pclass']].head(3))\n----&gt; 3 print('[] 안에 있는 숫자 index는 KeyError 오류 발생',titanic_df[0])\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-&gt; 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 0\n# 하지만 슬라이싱은 가능함\ntitanic_df[0:2]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr....\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mr...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntitanic_df[titanic_df['Pclass']==3].head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr....\nmale\n22.0\n1\n0\nA/5 21171\n7.250\nNaN\nS\n\n\n2\n3\n1\n3\nHeikkinen, ...\nfemale\n26.0\n0\n0\nSTON/O2. 31...\n7.925\nNaN\nS\n\n\n4\n5\n0\n3\nAllen, Mr. ...\nmale\n35.0\n0\n0\n373450\n8.050\nNaN\nS\ndata = {'Name': ['Chumin', 'Eunkyung', 'Jinwoong', 'Soobeom'],\n       'Year': [2011, 2016, 2015, 2015],\n       'Gender': ['M','F','M','M']}\ndata_df = pd.DataFrame(data,index=['1','2','3','4'])\ndata_df\n\n\n\n\n\n\n\n\nName\nYear\nGender\n\n\n\n\n1\nChumin\n2011\nM\n\n\n2\nEunkyung\n2016\nF\n\n\n3\nJinwoong\n2015\nM\n\n\n4\nSoobeom\n2015\nM\ndata_df.iloc[0,0] # 위치 기반!\n\n'Chumin'\n# 위치기반이기 때문에 칼럼명(문자) 입력시에 오류가 발생함\ndata_df.iloc[0, 'Name']\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:966, in _LocationIndexer._validate_tuple_indexer(self, key)\n    965 try:\n--&gt; 966     self._validate_key(k, i)\n    967 except ValueError as err:\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1614, in _iLocIndexer._validate_key(self, key, axis)\n   1613 else:\n-&gt; 1614     raise ValueError(f\"Can only index by location with a [{self._valid_types}]\")\n\nValueError: Can only index by location with a [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array]\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nCell In[72], line 2\n      1 # 위치기반이기 때문에 칼럼명(문자) 입력시에 오류가 발생함\n----&gt; 2 data_df.iloc[0, 'Name']\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1184, in _LocationIndexer.__getitem__(self, key)\n   1182     if self._is_scalar_access(key):\n   1183         return self.obj._get_value(*key, takeable=self._takeable)\n-&gt; 1184     return self._getitem_tuple(key)\n   1185 else:\n   1186     # we by definition only have the 0th axis\n   1187     axis = self.axis or 0\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1690, in _iLocIndexer._getitem_tuple(self, tup)\n   1689 def _getitem_tuple(self, tup: tuple):\n-&gt; 1690     tup = self._validate_tuple_indexer(tup)\n   1691     with suppress(IndexingError):\n   1692         return self._getitem_lowerdim(tup)\n\nFile ~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:968, in _LocationIndexer._validate_tuple_indexer(self, key)\n    966         self._validate_key(k, i)\n    967     except ValueError as err:\n--&gt; 968         raise ValueError(\n    969             \"Location based indexing can only have \"\n    970             f\"[{self._valid_types}] types\"\n    971         ) from err\n    972 return key\n\nValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types\nprint('\\n 맨 마지막 칼럼 데이터 [:, -1]\\n', data_df.iloc[:, -1])\nprint('\\n 맨 마지막 칼럼을 제외한 모든 데이터 [:, :-1]\\n', data_df.iloc[:, :-1])\n\n\n 맨 마지막 칼럼 데이터 [:, -1]\n 1    M\n2    F\n3    M\n4    M\nName: Gender, dtype: object\n\n 맨 마지막 칼럼을 제외한 모든 데이터 [:, :-1]\n        Name  Year\n1    Chumin  2011\n2  Eunkyung  2016\n3  Jinwoong  2015\n4   Soobeom  2015\ndata_df.loc['1', 'Name']\n\n'Chumin'\n# 명칭 기반이므로 숫자 입력 시 오류 발생함.\ntitanic_df = pd.read_csv(r'C:\\Users\\HOME\\Downloads\\Lab 3\\Lab 3\\titanic_train.csv')\ntitanic_boolean = titanic_df[titanic_df['Age']&gt;60]\nprint(type(titanic_boolean))\ntitanic_boolean\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n33\n34\n0\n2\nWheadon, Mr...\nmale\n66.0\n0\n0\nC.A. 24579\n10.5000\nNaN\nS\n\n\n54\n55\n0\n1\nOstby, Mr. ...\nmale\n65.0\n0\n1\n113509\n61.9792\nB30\nC\n\n\n96\n97\n0\n1\nGoldschmidt...\nmale\n71.0\n0\n0\nPC 17754\n34.6542\nA5\nC\n\n\n116\n117\n0\n3\nConnors, Mr...\nmale\n70.5\n0\n0\n370369\n7.7500\nNaN\nQ\n\n\n170\n171\n0\n1\nVan der hoe...\nmale\n61.0\n0\n0\n111240\n33.5000\nB19\nS\n\n\n252\n253\n0\n1\nStead, Mr. ...\nmale\n62.0\n0\n0\n113514\n26.5500\nC87\nS\n\n\n275\n276\n1\n1\nAndrews, Mi...\nfemale\n63.0\n1\n0\n13502\n77.9583\nD7\nS\n\n\n280\n281\n0\n3\nDuane, Mr. ...\nmale\n65.0\n0\n0\n336439\n7.7500\nNaN\nQ\n\n\n326\n327\n0\n3\nNysveen, Mr...\nmale\n61.0\n0\n0\n345364\n6.2375\nNaN\nS\n\n\n438\n439\n0\n1\nFortune, Mr...\nmale\n64.0\n1\n4\n19950\n263.0000\nC23 C25 C27\nS\n\n\n456\n457\n0\n1\nMillet, Mr....\nmale\n65.0\n0\n0\n13509\n26.5500\nE38\nS\n\n\n483\n484\n1\n3\nTurkula, Mr...\nfemale\n63.0\n0\n0\n4134\n9.5875\nNaN\nS\n\n\n493\n494\n0\n1\nArtagaveyti...\nmale\n71.0\n0\n0\nPC 17609\n49.5042\nNaN\nC\n\n\n545\n546\n0\n1\nNicholson, ...\nmale\n64.0\n0\n0\n693\n26.0000\nNaN\nS\n\n\n555\n556\n0\n1\nWright, Mr....\nmale\n62.0\n0\n0\n113807\n26.5500\nNaN\nS\n\n\n570\n571\n1\n2\nHarris, Mr....\nmale\n62.0\n0\n0\nS.W./PP 752\n10.5000\nNaN\nS\n\n\n625\n626\n0\n1\nSutton, Mr....\nmale\n61.0\n0\n0\n36963\n32.3208\nD50\nS\n\n\n630\n631\n1\n1\nBarkworth, ...\nmale\n80.0\n0\n0\n27042\n30.0000\nA23\nS\n\n\n672\n673\n0\n2\nMitchell, M...\nmale\n70.0\n0\n0\nC.A. 24580\n10.5000\nNaN\nS\n\n\n745\n746\n0\n1\nCrosby, Cap...\nmale\n70.0\n1\n1\nWE/P 5735\n71.0000\nB22\nS\n\n\n829\n830\n1\n1\nStone, Mrs....\nfemale\n62.0\n0\n0\n113572\n80.0000\nB28\nNaN\n\n\n851\n852\n0\n3\nSvensson, M...\nmale\n74.0\n0\n0\n347060\n7.7750\nNaN\nS\ntitanic_df.loc[titanic_df['Age']&gt;60, ['Name', 'Age']].head(3)\n\n\n\n\n\n\n\n\nName\nAge\n\n\n\n\n33\nWheadon, Mr...\n66.0\n\n\n54\nOstby, Mr. ...\n65.0\n\n\n96\nGoldschmidt...\n71.0"
  },
  {
    "objectID": "notebooks/ML/1장 공부.html#isna로-결손-데이터-여부-확인",
    "href": "notebooks/ML/1장 공부.html#isna로-결손-데이터-여부-확인",
    "title": "데이터 셀렉션 및 필터링",
    "section": "isna()로 결손 데이터 여부 확인",
    "text": "isna()로 결손 데이터 여부 확인\n\n그냥 isna()를 입력하면 해당 칼럼이 “모두” Null 값인지 확인가능\nNull값의 개수를 확인하고 싶을 때는 isna().sum()\n\n\ntitanic_df.isna().head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\ntitanic_df.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64"
  },
  {
    "objectID": "notebooks/ML/1장 공부.html#fillna로-결손-데이터-대체하기",
    "href": "notebooks/ML/1장 공부.html#fillna로-결손-데이터-대체하기",
    "title": "데이터 셀렉션 및 필터링",
    "section": "fillna()로 결손 데이터 대체하기",
    "text": "fillna()로 결손 데이터 대체하기\n\ntitanic_df['Cabin']=titanic_df['Cabin'].fillna('C000')\ntitanic_df.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr....\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nC000\nS\n\n\n1\n2\n1\n1\nCumings, Mr...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, ...\nfemale\n26.0\n0\n0\nSTON/O2. 31...\n7.9250\nC000\nS\n\n\n\n\n\n\n\n\ntitanic_df['Age']=titanic_df['Age'].fillna(titanic_df['Age'].mean()) # 평균값으로 대체함.\ntitanic_df['Embarked']=titanic_df['Embarked'].fillna('S')\ntitanic_df.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Write a short bio, interests, and contact info here.\n\nUniversity / Major\nInterests (Generative AI, Data Engineering, Cloud AI, etc.)\nLinks (GitHub, LinkedIn, Email)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "👋 Welcome\nThis is your AI portfolio & study blog built with Quarto and GitHub Pages.\n\nPut your Jupyter notebooks under notebooks/ (e.g., notebooks/ML/week1_intro.ipynb)\nThey will appear on the Study Notes page automatically after you push to GitHub.\nCustomize the site in _quarto.yml."
  },
  {
    "objectID": "notebooks/ML/2.6 사이킷런_타이타닉.html",
    "href": "notebooks/ML/2.6 사이킷런_타이타닉.html",
    "title": "교차검증 수행",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntitanic_df = pd.read_csv(r\"C:\\Users\\HOME\\Downloads\\archive\\Titanic-Dataset.csv\")\ntitanic_df.head(3)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n\n\n\n\n\n\nprint(\"데이터 정보\")\nprint(\"\")\nprint(titanic_df.info())\n\n데이터 정보\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nNone\n\n\n\ntitanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)\ntitanic_df['Cabin'].fillna('N', inplace=True)\ntitanic_df['Embarked'].fillna('N', inplace=True)\nprint(\"Null 갯수\", titanic_df.isnull().sum().sum())\n\nNull 갯수 0\n\n\n\nprint('Sex 값 분포:\\n', titanic_df['Sex'].value_counts())\nprint('\\n Cabin 값 분포: \\n', titanic_df['Cabin'].value_counts())\nprint('\\n Embarked 값 분포: \\n', titanic_df['Embarked'].value_counts())\n# Cabin 칼럼 데이터의 경우 정리가 이상하게 되어 있음.\n# 앞자리 알파벳(등급)이 중요한 의미를 가지므로 데이터를 처리해보자.\n\nSex 값 분포:\n Sex\nmale      577\nfemale    314\nName: count, dtype: int64\n\n Cabin 값 분포: \n Cabin\nN              687\nC23 C25 C27      4\nG6               4\nB96 B98          4\nC22 C26          3\n              ... \nE34              1\nC7               1\nC54              1\nE36              1\nC148             1\nName: count, Length: 148, dtype: int64\n\n Embarked 값 분포: \n Embarked\nS    644\nC    168\nQ     77\nN      2\nName: count, dtype: int64\n\n\n\n# Cabin 데이터 값의 앞글자만 떼어내기\ntitanic_df['Cabin'] = titanic_df['Cabin'].str[:1]\nprint(titanic_df['Cabin'].head(3))\n\n0    N\n1    C\n2    N\nName: Cabin, dtype: object\n\n\n\ntitanic_df.groupby(['Sex', 'Survived'])['Survived'].count()\n\nSex     Survived\nfemale  0            81\n        1           233\nmale    0           468\n        1           109\nName: Survived, dtype: int64\n\n\n\nimport seaborn as sns\n# 시본 클래스를 이용해서 성별에 따른 생존율 시각화\nsns.barplot(x='Sex', y='Survived', data=titanic_df)\n\n\n\n\n\n\n\n\n\n# 좌석 등급과 성별에 따른 생존율 \nsns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df)\n\n\n\n\n\n\n\n\n\n# 입력 age에 따라 구분값을 반환하는 함수 설정. DF의 apply lambda식에 사용\ndef get_category(age):\n    cat=''\n    if age &lt;= -1: cat='Unknown'\n    elif age&lt;=5: cat='Baby'\n    elif age&lt;=12: cat='Child'\n    elif age&lt;=18: cat='Teenager'\n    elif age&lt;=25: cat='Student'\n    elif age&lt;=35: cat='Young Adult'\n    elif age&lt;=60: cat='Adult'\n    else: cat='Elderly'\n\n    return cat\n\n# 막대그래프의 크기 figure를 더 크게 설정\nplt.figure(figsize=(10,6))\n\n# X축의 값을 순차적으로 표시하기 위한 설정\ngroup_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']\n\n# lambda 식에 위에서 생성한 함수를 반환값으로 지정\n# 함수 입력값으로 age 컬럼값을 받아서 해당하는 cat 반환\ntitanic_df['Age_cat']=titanic_df['Age'].apply(lambda x: get_category(x))\nsns.barplot(x='Age_cat', y='Survived', hue='Sex', data=titanic_df, order=group_names)\ntitanic_df.drop('Age_cat', axis=1, inplace=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn import preprocessing\n\ndef encode_features(dataDF):\n    features = ['Cabin', 'Sex', 'Embarked']\n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(dataDF[feature])\n        dataDF[feature] = le.transform(dataDF[feature])\n\n    return dataDF\ntitanic_df = encode_features(titanic_df)\ntitanic_df.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\n1\n22.0\n1\n0\nA/5 21171\n7.2500\n7\n3\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n0\n38.0\n1\n0\nPC 17599\n71.2833\n2\n0\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\n0\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\n7\n3\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n0\n35.0\n1\n0\n113803\n53.1000\n2\n3\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\n1\n35.0\n0\n0\n373450\n8.0500\n7\n3\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Null 처리 함수\ndef fillna(df):\n    df['Age'] = df['Age'].fillna(df['Age'].mean())\n    df['Cabin'] = df['Cabin'].fillna('N')\n    df['Embarked'] = df['Embarked'].fillna('N')\n    df['Fare'] = df['Fare'].fillna(0)\n\n    return df\n\n# 머신러닝 알고리즘에 불필요한 피처 제거\ndef drop_features(df):\n    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n    return df\n\n# 레이블 인코딩 수행\ndef format_features(df):\n    df['Cabin'] = df['Cabin'].str[:1]\n    features = ['Cabin', 'Sex', 'Embarked']\n    for feature in features:\n        le = LabelEncoder()\n        le = le.fit(df[feature])\n        df[feature] = le.transform(df[feature])\n    return df\n\n# 앞에서 설정한 데이터 전처리 함수 호출\ndef transform_features(df):\n    df = fillna(df)\n    df = drop_features(df)\n    df = format_features(df)\n    return df\n\n\ntitanic_df = pd.read_csv(r\"C:\\Users\\HOME\\Downloads\\archive\\Titanic-Dataset.csv\")\ny_titanic_df = titanic_df['Survived']\nX_titanic_df = titanic_df.drop('Survived', axis=1)\n\nX_titanic_df = transform_features(X_titanic_df)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df,\n                                                   test_size=0.2, random_state=11)\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# 분류 클래스 생성\ndt_clf = DecisionTreeClassifier()\nrf_clf = RandomForestClassifier()\nlr_clf = LogisticRegression()\n\n# DT 학습 예측 평가\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)\nprint('DT 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\n\n# RF 학습 예측 평가\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nprint('RF 정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))\n\n# LR 학습 예측 평가\nlr_clf.fit(X_train, y_train)\nlr_pred = lr_clf.predict(X_test)\nprint('LR 정확도: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))\n\nDT 정확도: 0.8045\nRF 정확도: 0.8436\nLR 정확도: 0.8492\n\n\n\nfrom sklearn.model_selection import KFold\n\ndef exec_kfold(clf, folds=5):\n    kfold = KFold(n_splits=folds)\n    scores=[]\n\n    for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):\n        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]\n        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]\n\n        clf.fit(X_train, y_train)\n        predictions = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        scores.append(accuracy)\n        print(\"교차 검증 {0} 정확도: {1: 4f}\".format(iter_count, accuracy))\n\n    mean_score = np.mean(scores)\n    print(\"평균 정확도: {0:.4f}\".format(mean_score))\n\nexec_kfold(dt_clf)\n\n교차 검증 0 정확도:  0.748603\n교차 검증 1 정확도:  0.764045\n교차 검증 2 정확도:  0.797753\n교차 검증 3 정확도:  0.775281\n교차 검증 4 정확도:  0.825843\n평균 정확도: 0.7823\n\n\n\n최적 변수 찾기\n\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'max_depth': [2,3,5,10],\n             'min_samples_split': [2,3,5], 'min_samples_leaf': [1,5,8]}\n\ngrid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring='accuracy', cv=5)\ngrid_dclf.fit(X_train, y_train)\n\nprint('GridSearchCV 최적 하이퍼 파라미터: ', grid_dclf.best_params_)\nprint('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_))\nbest_dclf = grid_dclf.best_estimator_\n\n# 최적 변수로 다시 예측 및 평가 수행\ndpredictions = best_dclf.predict(X_test)\naccuracy = accuracy_score(y_test, dpredictions)\nprint('테스트 세트에서의 정확도: {0:.4f}'.format(accuracy))\n\nGridSearchCV 최적 하이퍼 파라미터:  {'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 2}\nGridSearchCV 최고 정확도: 0.7992\n테스트 세트에서의 정확도: 0.8715"
  },
  {
    "objectID": "notebooks/ML/3장.평가.html",
    "href": "notebooks/ML/3장.평가.html",
    "title": "숫자 데이터 활용",
    "section": "",
    "text": "import sklearn\nprint(sklearn.__version__)\n\n1.4.2\nimport numpy as np\nfrom sklearn.base import BaseEstimator\n\nclass MyDummyClassifier(BaseEstimator):\n    # fit()은 아무것도 학습하지 않음.\n    def fit(self, X, y=None):\n        pass\n    # predict()는 아무 단순하게 Sex feature가 1-&gt;0, 그렇지 않으면 1로 예측\n    def predict(self, X):\n        pred = np.zeros((X.shape[0], 1))\n        for i in range(X.shape[0]):\n            if X['Sex'].iloc[i] == 1:\n                pred[i] = 0\n            else:\n                pred[i] = 1\n        return pred\nfrom sklearn.preprocessing import LabelEncoder\n\n# Null 처리 함수\ndef fillna(df):\n    df['Age'] = df['Age'].fillna(df['Age'].mean())\n    df['Cabin'] = df['Cabin'].fillna('N')\n    df['Embarked'] = df['Embarked'].fillna('N')\n    df['Fare'] = df['Fare'].fillna(0)\n\n    return df\n\n# 머신러닝 알고리즘에 불필요한 피처 제거\ndef drop_features(df):\n    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n    return df\n\n# 레이블 인코딩 수행\ndef format_features(df):\n    df['Cabin'] = df['Cabin'].str[:1]\n    features = ['Cabin', 'Sex', 'Embarked']\n    for feature in features:\n        le = LabelEncoder()\n        le = le.fit(df[feature])\n        df[feature] = le.transform(df[feature])\n    return df\n\n# 앞에서 설정한 데이터 전처리 함수 호출\ndef transform_features(df):\n    df = fillna(df)\n    df = drop_features(df)\n    df = format_features(df)\n    return df\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ntitanic_df = pd.read_csv(r\"C:\\Users\\HOME\\Downloads\\archive\\Titanic-Dataset.csv\")\ny_titanic_df = titanic_df['Survived']\nX_titanic_df = titanic_df.drop('Survived', axis=1)\nX_titanic_df = transform_features(X_titanic_df)\nX_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size=0.2)\n\nmyclf = MyDummyClassifier()\nmyclf.fit(X_train, y_train)\n\nmypredictions = myclf.predict(X_test)\nprint(\"myDummy의 정확도는: {0:.4f}\".format(accuracy_score(y_test, mypredictions)))\n\nmyDummy의 정확도는: 0.8268\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport pandas as pd\n\nclass MyFakeClassifier(BaseEstimator):\n    def fit(self, X, y):\n        pass\n    # 입력값으로 들어오는 X데이터 셋의 크기만큼 모두 0으로 만들어 반환\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n# 사이킷런의 내장 데이터셋을 이용해 MNIST 데이터 로딩\ndigits = load_digits()\n\nprint(digits.data)\nprint(\"데이터 shape: \", digits.data.shape)\nprint(digits.target)\nprint(\"데이터 타겟 shape: \", digits.target.shape)\n\n[[ 0.  0.  5. ...  0.  0.  0.]\n [ 0.  0.  0. ... 10.  0.  0.]\n [ 0.  0.  0. ... 16.  9.  0.]\n ...\n [ 0.  0.  1. ...  6.  0.  0.]\n [ 0.  0.  2. ... 12.  0.  0.]\n [ 0.  0. 10. ... 12.  1.  0.]]\n데이터 shape:  (1797, 64)\n[0 1 2 ... 8 9 8]\n데이터 타겟 shape:  (1797,)\ndigits.target == 7\n\narray([False, False, False, ..., False, False, False])\ny = (digits.target==7).astype(int)\nX_train, X_test, y_train, y_test = train_test_split(digits.data, y)\n# 불균형한 레이블 데이터 분포도 확인\nprint(y_test.shape)\nprint(\"0과 1의 분포도\")\nprint(pd.Series(y_test).value_counts())\n\nfakeclf = MyFakeClassifier()\nfakeclf.fit(X_train, y_train)\nfakepred = fakeclf.predict(X_test)\nprint(accuracy_score(y_test, fakepred))\n# 전부 0으로 예측해도 90%의 정확도가 나옴.\n\n(450,)\n0과 1의 분포도\n0    409\n1     41\nName: count, dtype: int64\n0.9088888888888889"
  },
  {
    "objectID": "notebooks/ML/3장.평가.html#roc-curve",
    "href": "notebooks/ML/3장.평가.html#roc-curve",
    "title": "숫자 데이터 활용",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nX축은 FPR, y축은 TPR\n\n\nfrom sklearn.metrics import roc_curve\n\npred_proba_class1 = lr_clf.predict_proba(X_test)[:, 1]\n\nfprs, tprs, thresholds = roc_curve(y_test, pred_proba_class1we23)\nthr_index = np.arange(1, thresholds.shape[0], 5)"
  },
  {
    "objectID": "notebooks/ML/4.3. 앙상블.html",
    "href": "notebooks/ML/4.3. 앙상블.html",
    "title": "앙상블 학습 개요",
    "section": "",
    "text": "여러 개의 classifier를 생성하고 그 예측을 결합하여 최종 예측을 도출하는 기법\n정형 데이터 분류 시에 뛰어난 성능 ## 앙상블 방식의 유형 ### 1. 보팅\n여러 개의 분류기가 투표를 통해 최종 예측을 결정하는 방식\n서로 다른 알고리즘을 가진 분류기 결합 ### 2. 배깅\n여러 개의 분류기가 투표를 통해 최종 예측을 결정하는 방식\n모두 같은 유형의 알고리즘 기반\n데이터 샘플링을 다양하게 학습\nex. 랜덤 포레스트 알고리즘\nBootstrapping: 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식, 중첩 허용 ### 3. 부스팅 ### 4. 스태킹 앙상블"
  },
  {
    "objectID": "notebooks/ML/4.3. 앙상블.html#hard-voting",
    "href": "notebooks/ML/4.3. 앙상블.html#hard-voting",
    "title": "앙상블 학습 개요",
    "section": "Hard Voting",
    "text": "Hard Voting\n\n에측한 결괏값들 중 다수의 분류기가 결정한 예측값 -&gt; 최종 ## Soft Voting\n분류기들의 결정값의 평균 -&gt; 최종\n소프트보팅의 예측 성능이 더 좋음"
  },
  {
    "objectID": "notebooks/ML/4.3. 앙상블.html#클래스-이름명-참조-방법",
    "href": "notebooks/ML/4.3. 앙상블.html#클래스-이름명-참조-방법",
    "title": "앙상블 학습 개요",
    "section": "클래스 이름명 참조 방법",
    "text": "클래스 이름명 참조 방법\n\nclassifier\n\n이미 생성된 객체(예: 머신러닝 모델 인스턴스, DecisionTreeClassifier() 같은 것).\n\nclassifier.__class__\n\n해당 객체가 어떤 클래스에서 만들어졌는지 나타내는 클래스 자체를 반환.\n예: DecisionTreeClassifier 클래스\n\nclassifier.__class__.__name__\n\n그 클래스의 이름(문자열)을 반환.\n예: “DecisionTreeClassifier”\n\nclass_name = …\n\n그 문자열을 class_name 변수에 저장."
  },
  {
    "objectID": "notebooks/ML/4.7.LightGBM.html",
    "href": "notebooks/ML/4.7.LightGBM.html",
    "title": "LightGBM 적용 - 위스콘신 Breast Cancer Prediction",
    "section": "",
    "text": "XGBoost보다 학습에 걸리는 시간이 훨씬 적고, 메모리 사용량도 적다.\n단점: 과적합 발생 쉽다. # 리프 중심 트리 분할 (Leaf Wise)\n대부분 트리 기반 알고리즘은 균형 트리 분할(Level Wise) 방식 사용 -&gt; 오버피팅에 강한 구조\nLeaf Wise는 최대 손실 값을 가지는 리프 노드를 지속적으로 분할\n빠른 시간 # LGBM 하이퍼 파라미터\nnum_iterations [default=100]\nlearning_rate[default=0.1]\nlambda_l2, lambda_l1 ## 하이퍼 파라미터 튜닝 방안\nnum_leaves의 개수를 중심으로 min_child_samples, max_depth를 함께 조정하며 모델의 복잡도를 줄인다.\n\n\n!pip install lightgbm==3.3.2\n\nRequirement already satisfied: lightgbm==3.3.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (3.3.2)\nRequirement already satisfied: wheel in c:\\users\\home\\anaconda3\\lib\\site-packages (from lightgbm==3.3.2) (0.43.0)\nRequirement already satisfied: numpy in c:\\users\\home\\anaconda3\\lib\\site-packages (from lightgbm==3.3.2) (1.26.4)\nRequirement already satisfied: scipy in c:\\users\\home\\anaconda3\\lib\\site-packages (from lightgbm==3.3.2) (1.13.1)\nRequirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from lightgbm==3.3.2) (1.4.2)\nRequirement already satisfied: joblib&gt;=1.2.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0-&gt;lightgbm==3.3.2) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0-&gt;lightgbm==3.3.2) (2.2.0)\n\n\n\nimport lightgbm\nprint(lightgbm.__version__)\n\n4.5.0\n\n\n\nLightGBM 적용 - 위스콘신 Breast Cancer Prediction\n\n# LightGBM의 파이썬 패키지인 lightgbm에서 LGBMClassifier 임포트\nfrom lightgbm import LGBMClassifier\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\ndataset = load_breast_cancer()\n\ncancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\ncancer_df['target']= dataset.target\nX_features = cancer_df.iloc[:, :-1]\ny_label = cancer_df.iloc[:, -1]\n\n# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\nX_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156 )\n\n# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검증용 데이터로 분리\nX_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train, test_size=0.1, random_state=156 )\n\n# 앞서 XGBoost와 동일하게 n_estimators는 400 설정.\nlgbm_wrapper = LGBMClassifier(n_estimators=400, learning_rate=0.05)\n\n# LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능.\nevals = [(X_tr, y_tr), (X_val, y_val)]\nlgbm_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric=\"logloss\", eval_set=evals, verbose=True)\npreds = lgbm_wrapper.predict(X_test)\npred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]\n\n[1] training's binary_logloss: 0.625671 valid_1's binary_logloss: 0.628248\n[2] training's binary_logloss: 0.588173 valid_1's binary_logloss: 0.601106\n[3] training's binary_logloss: 0.554518 valid_1's binary_logloss: 0.577587\n[4] training's binary_logloss: 0.523972 valid_1's binary_logloss: 0.556324\n[5] training's binary_logloss: 0.49615  valid_1's binary_logloss: 0.537407\n[6] training's binary_logloss: 0.470108 valid_1's binary_logloss: 0.519401\n[7] training's binary_logloss: 0.446647 valid_1's binary_logloss: 0.502637\n[8] training's binary_logloss: 0.425055 valid_1's binary_logloss: 0.488311\n[9] training's binary_logloss: 0.405125 valid_1's binary_logloss: 0.474664\n[10]    training's binary_logloss: 0.386526 valid_1's binary_logloss: 0.461267\n[11]    training's binary_logloss: 0.367027 valid_1's binary_logloss: 0.444274\n[12]    training's binary_logloss: 0.350713 valid_1's binary_logloss: 0.432755\n[13]    training's binary_logloss: 0.334601 valid_1's binary_logloss: 0.421371\n[14]    training's binary_logloss: 0.319854 valid_1's binary_logloss: 0.411418\n[15]    training's binary_logloss: 0.306374 valid_1's binary_logloss: 0.402989\n[16]    training's binary_logloss: 0.293116 valid_1's binary_logloss: 0.393973\n[17]    training's binary_logloss: 0.280812 valid_1's binary_logloss: 0.384801\n[18]    training's binary_logloss: 0.268352 valid_1's binary_logloss: 0.376191\n[19]    training's binary_logloss: 0.256942 valid_1's binary_logloss: 0.368378\n[20]    training's binary_logloss: 0.246443 valid_1's binary_logloss: 0.362062\n[21]    training's binary_logloss: 0.236874 valid_1's binary_logloss: 0.355162\n[22]    training's binary_logloss: 0.227501 valid_1's binary_logloss: 0.348933\n[23]    training's binary_logloss: 0.218988 valid_1's binary_logloss: 0.342819\n[24]    training's binary_logloss: 0.210621 valid_1's binary_logloss: 0.337386\n[25]    training's binary_logloss: 0.202076 valid_1's binary_logloss: 0.331523\n[26]    training's binary_logloss: 0.194199 valid_1's binary_logloss: 0.326349\n[27]    training's binary_logloss: 0.187107 valid_1's binary_logloss: 0.322785\n[28]    training's binary_logloss: 0.180535 valid_1's binary_logloss: 0.317877\n[29]    training's binary_logloss: 0.173834 valid_1's binary_logloss: 0.313928\n[30]    training's binary_logloss: 0.167198 valid_1's binary_logloss: 0.310105\n[31]    training's binary_logloss: 0.161229 valid_1's binary_logloss: 0.307107\n[32]    training's binary_logloss: 0.155494 valid_1's binary_logloss: 0.303837\n[33]    training's binary_logloss: 0.149125 valid_1's binary_logloss: 0.300315\n[34]    training's binary_logloss: 0.144045 valid_1's binary_logloss: 0.297816\n[35]    training's binary_logloss: 0.139341 valid_1's binary_logloss: 0.295387\n[36]    training's binary_logloss: 0.134625 valid_1's binary_logloss: 0.293063\n[37]    training's binary_logloss: 0.129167 valid_1's binary_logloss: 0.289127\n[38]    training's binary_logloss: 0.12472  valid_1's binary_logloss: 0.288697\n[39]    training's binary_logloss: 0.11974  valid_1's binary_logloss: 0.28576\n[40]    training's binary_logloss: 0.115054 valid_1's binary_logloss: 0.282853\n[41]    training's binary_logloss: 0.110662 valid_1's binary_logloss: 0.279441\n[42]    training's binary_logloss: 0.106358 valid_1's binary_logloss: 0.28113\n[43]    training's binary_logloss: 0.102324 valid_1's binary_logloss: 0.279139\n[44]    training's binary_logloss: 0.0985699    valid_1's binary_logloss: 0.276465\n[45]    training's binary_logloss: 0.094858 valid_1's binary_logloss: 0.275946\n[46]    training's binary_logloss: 0.0912486    valid_1's binary_logloss: 0.272819\n[47]    training's binary_logloss: 0.0883115    valid_1's binary_logloss: 0.272306\n[48]    training's binary_logloss: 0.0849963    valid_1's binary_logloss: 0.270452\n[49]    training's binary_logloss: 0.0821742    valid_1's binary_logloss: 0.268671\n[50]    training's binary_logloss: 0.0789991    valid_1's binary_logloss: 0.267587\n[51]    training's binary_logloss: 0.0761072    valid_1's binary_logloss: 0.26626\n[52]    training's binary_logloss: 0.0732567    valid_1's binary_logloss: 0.265542\n[53]    training's binary_logloss: 0.0706388    valid_1's binary_logloss: 0.264547\n[54]    training's binary_logloss: 0.0683911    valid_1's binary_logloss: 0.26502\n[55]    training's binary_logloss: 0.0659347    valid_1's binary_logloss: 0.264388\n[56]    training's binary_logloss: 0.0636873    valid_1's binary_logloss: 0.263128\n[57]    training's binary_logloss: 0.0613354    valid_1's binary_logloss: 0.26231\n[58]    training's binary_logloss: 0.0591944    valid_1's binary_logloss: 0.262011\n[59]    training's binary_logloss: 0.057033 valid_1's binary_logloss: 0.261454\n[60]    training's binary_logloss: 0.0550801    valid_1's binary_logloss: 0.260746\n[61]    training's binary_logloss: 0.0532381    valid_1's binary_logloss: 0.260236\n[62]    training's binary_logloss: 0.0514074    valid_1's binary_logloss: 0.261586\n[63]    training's binary_logloss: 0.0494837    valid_1's binary_logloss: 0.261797\n[64]    training's binary_logloss: 0.0477826    valid_1's binary_logloss: 0.262533\n[65]    training's binary_logloss: 0.0460364    valid_1's binary_logloss: 0.263305\n[66]    training's binary_logloss: 0.0444552    valid_1's binary_logloss: 0.264072\n[67]    training's binary_logloss: 0.0427638    valid_1's binary_logloss: 0.266223\n[68]    training's binary_logloss: 0.0412449    valid_1's binary_logloss: 0.266817\n[69]    training's binary_logloss: 0.0398589    valid_1's binary_logloss: 0.267819\n[70]    training's binary_logloss: 0.0383095    valid_1's binary_logloss: 0.267484\n[71]    training's binary_logloss: 0.0368803    valid_1's binary_logloss: 0.270233\n[72]    training's binary_logloss: 0.0355637    valid_1's binary_logloss: 0.268442\n[73]    training's binary_logloss: 0.0341747    valid_1's binary_logloss: 0.26895\n[74]    training's binary_logloss: 0.0328302    valid_1's binary_logloss: 0.266958\n[75]    training's binary_logloss: 0.0317853    valid_1's binary_logloss: 0.268091\n[76]    training's binary_logloss: 0.0305626    valid_1's binary_logloss: 0.266419\n[77]    training's binary_logloss: 0.0295001    valid_1's binary_logloss: 0.268588\n[78]    training's binary_logloss: 0.0284699    valid_1's binary_logloss: 0.270964\n[79]    training's binary_logloss: 0.0273953    valid_1's binary_logloss: 0.270293\n[80]    training's binary_logloss: 0.0264668    valid_1's binary_logloss: 0.270523\n[81]    training's binary_logloss: 0.0254636    valid_1's binary_logloss: 0.270683\n[82]    training's binary_logloss: 0.0245911    valid_1's binary_logloss: 0.273187\n[83]    training's binary_logloss: 0.0236486    valid_1's binary_logloss: 0.275994\n[84]    training's binary_logloss: 0.0228047    valid_1's binary_logloss: 0.274053\n[85]    training's binary_logloss: 0.0221693    valid_1's binary_logloss: 0.273211\n[86]    training's binary_logloss: 0.0213043    valid_1's binary_logloss: 0.272626\n[87]    training's binary_logloss: 0.0203934    valid_1's binary_logloss: 0.27534\n[88]    training's binary_logloss: 0.0195552    valid_1's binary_logloss: 0.276228\n[89]    training's binary_logloss: 0.0188623    valid_1's binary_logloss: 0.27525\n[90]    training's binary_logloss: 0.0183664    valid_1's binary_logloss: 0.276485\n[91]    training's binary_logloss: 0.0176788    valid_1's binary_logloss: 0.277052\n[92]    training's binary_logloss: 0.0170059    valid_1's binary_logloss: 0.277686\n[93]    training's binary_logloss: 0.0164317    valid_1's binary_logloss: 0.275332\n[94]    training's binary_logloss: 0.015878 valid_1's binary_logloss: 0.276236\n[95]    training's binary_logloss: 0.0152959    valid_1's binary_logloss: 0.274538\n[96]    training's binary_logloss: 0.0147216    valid_1's binary_logloss: 0.275244\n[97]    training's binary_logloss: 0.0141758    valid_1's binary_logloss: 0.275829\n[98]    training's binary_logloss: 0.0136551    valid_1's binary_logloss: 0.276654\n[99]    training's binary_logloss: 0.0131585    valid_1's binary_logloss: 0.277859\n[100]   training's binary_logloss: 0.0126961    valid_1's binary_logloss: 0.279265\n[101]   training's binary_logloss: 0.0122421    valid_1's binary_logloss: 0.276695\n[102]   training's binary_logloss: 0.0118067    valid_1's binary_logloss: 0.278488\n[103]   training's binary_logloss: 0.0113994    valid_1's binary_logloss: 0.278932\n[104]   training's binary_logloss: 0.0109799    valid_1's binary_logloss: 0.280997\n[105]   training's binary_logloss: 0.0105953    valid_1's binary_logloss: 0.281454\n[106]   training's binary_logloss: 0.0102381    valid_1's binary_logloss: 0.282058\n[107]   training's binary_logloss: 0.00986714   valid_1's binary_logloss: 0.279275\n[108]   training's binary_logloss: 0.00950998   valid_1's binary_logloss: 0.281427\n[109]   training's binary_logloss: 0.00915965   valid_1's binary_logloss: 0.280752\n[110]   training's binary_logloss: 0.00882581   valid_1's binary_logloss: 0.282152\n[111]   training's binary_logloss: 0.00850714   valid_1's binary_logloss: 0.280894\n\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score, roc_auc_score\n\ndef get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix( y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test , pred)\n    recall = recall_score(y_test , pred)\n    f1 = f1_score(y_test,pred)\n    # ROC-AUC 추가 \n    roc_auc = roc_auc_score(y_test, pred_proba)\n    print('오차 행렬')\n    print(confusion)\n    # ROC-AUC print 추가\n    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n\n\nget_clf_eval(y_test, preds, pred_proba)\n\n오차 행렬\n[[34  3]\n [ 2 75]]\n정확도: 0.9561, 정밀도: 0.9615, 재현율: 0.9740,    F1: 0.9677, AUC:0.9877\n\n\n\n# plot_importance()를 이용하여 feature 중요도 시각화\nfrom lightgbm import plot_importance\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots(figsize=(10,12))\nplot_importance(lgbm_wrapper, ax=ax)"
  },
  {
    "objectID": "notebooks/ML/4장.분류.html",
    "href": "notebooks/ML/4장.분류.html",
    "title": "결정 트리 과적합 (Overfitting)",
    "section": "",
    "text": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndt_clf = DecisionTreeClassifier()\n\niris_data = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, \n                                                   test_size=0.2)\ndt_clf.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier() \n\n\n\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(dt_clf, out_file='tree.dot', class_names = iris_data.target_names,\n                feature_names=iris_data.feature_names, impurity=True, filled=True)\n\n\npip install graphviz\n\nRequirement already satisfied: graphviz in c:\\users\\home\\anaconda3\\lib\\site-packages (0.21)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport graphviz\n\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\n\n# feature importance 추출\nprint(\"Feature importances: {0}\".format(np.round(dt_clf.feature_importances_,3)))\n\nfor name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):\n    print(\"{0} : {1:.3f}\".format(name, value))\n\nsns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) # petal width의 feature 중요도가 가장 높음!\n\nFeature importances: [0.023 0.    0.03  0.947]\nsepal length (cm) : 0.023\nsepal width (cm) : 0.000\npetal length (cm) : 0.030\npetal width (cm) : 0.947\n\n\n\n\n\n\n\n\n\n\nmake_classification() :분류를 위한 테스트용 데이터를 쉽게 만들게 해주는 함수\n\n\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.title(\"3 Class values with 2 Features Samplt data creation\")\n\nX_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                                          n_classes=3, n_clusters_per_class=1)\nplt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, cmap='rainbow', edgecolor='k')\n\n\n\n\n\n\n\n\n\n# 경계를 시각화하는 함수\nimport numpy as np\n\ndef visualize_boundary(model, X, y):\n    fig, ax = plt.subplots()\n\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',\n              clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    ax.axis('off')\n    xlim_start, xlim_end = ax.get_xlim()\n    ylim_start, ylim_end = ax.get_ylim()\n\n    model.fit(X,y)\n    xx, yy = np.meshgrid(np.linspace(xlim_start, xlim_end, num=200), np.linspace(ylim_start, ylim_end, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                          levels=np.arange(n_classes + 1) - 0.5,\n                          cmap='rainbow', clim=(y.min(), y.max()),\n                          zorder=1)\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_clf = DecisionTreeClassifier().fit(X_features, y_labels)\nvisualize_boundary(dt_clf, X_features, y_labels)\n\n\n\n\n\n\n\n\n\ndt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels)\nvisualize_boundary(dt_clf, X_features, y_labels)\n\n\n\n\n\n\n\n\n\n결정 트리 실습 - Human Activity Recognition\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfeature_name_df = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\features.txt\",\n                              sep=r'\\s+', header=None, names=['column_index', 'column_name'])\nfeature_name = feature_name_df.iloc[:, 1].values.tolist()\nprint(\"10개만 추출: \", feature_name[:10])\n\n10개만 추출:  ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z', 'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z', 'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z', 'tBodyAcc-max()-X']\n\n\n\nfeature_dup_df = feature_name_df.groupby('column_name').count()\nprint(feature_dup_df[feature_dup_df['column_index']&gt;1].count())\nfeature_dup_df[feature_dup_df['column_index']&gt;1].head()\n\ncolumn_index    42\ndtype: int64\n\n\n\n\n\n\n\n\n\ncolumn_index\n\n\ncolumn_name\n\n\n\n\n\nfBodyAcc-bandsEnergy()-1,16\n3\n\n\nfBodyAcc-bandsEnergy()-1,24\n3\n\n\nfBodyAcc-bandsEnergy()-1,8\n3\n\n\nfBodyAcc-bandsEnergy()-17,24\n3\n\n\nfBodyAcc-bandsEnergy()-17,32\n3\n\n\n\n\n\n\n\n\ndef get_new_feature_name_df(old_feature_name_df):\n    feature_dup_df = pd.DataFrame(\n        data=old_feature_name_df.groupby('column_name').cumcount(),\n        columns=['dup_cnt']\n    ).reset_index()\n\n    new_feature_name_df = pd.merge(\n        old_feature_name_df.reset_index(), feature_dup_df, how='outer'\n    )\n\n    # iloc 사용해서 FutureWarning 방지\n    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(\n        lambda x: x.iloc[0] + '_' + str(x.iloc[1]) if x.iloc[1] &gt; 0 else x.iloc[0],\n        axis=1\n    )\n\n    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n    return new_feature_name_df\n\n\nimport pandas as pd\n\ndef get_human_dataset( ):\n    \n    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\n    feature_name_df = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\features.txt\",sep=r'\\s+',\n                        header=None,names=['column_index','column_name'])\n    \n    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성. \n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    \n    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n    \n    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용\n    X_train = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\train\\X_train.txt\",sep=r'\\s+', names=feature_name )\n    X_test = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\test\\X_test.txt\",sep=r'\\s+', names=feature_name)\n    \n    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여\n    y_train = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\train\\y_train.txt\",sep=r'\\s+',header=None,names=['action'])\n    y_test = pd.read_csv(r\"C:\\Users\\HOME\\Desktop\\human_activity\\test\\y_test.txt\",sep=r'\\s+',header=None,names=['action'])\n    \n    # 로드된 학습/테스트용 DataFrame을 모두 반환 \n    return X_train, X_test, y_train, y_test\n\n\nX_train, X_test, y_train, y_test = get_human_dataset()\n\n\nprint('## 학습 피처 데이터셋 info()')\nprint(X_train.info())\n\n## 학습 피처 데이터셋 info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7352 entries, 0 to 7351\nColumns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean)\ndtypes: float64(561)\nmemory usage: 31.5 MB\nNone\n\n\n\nprint(y_train['action'].value_counts())\n\naction\n6    1407\n5    1374\n4    1286\n1    1226\n2    1073\n3     986\nName: count, dtype: int64\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정\ndt_clf = DecisionTreeClassifier(random_state=156)\ndt_clf.fit(X_train , y_train)\npred = dt_clf.predict(X_test)\naccuracy = accuracy_score(y_test , pred)\nprint('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))\n\n# DecisionTreeClassifier의 하이퍼 파라미터 추출\nprint('DecisionTreeClassifier 기본 하이퍼 파라미터:\\n', dt_clf.get_params())\n\n결정 트리 예측 정확도: 0.8548\nDecisionTreeClassifier 기본 하이퍼 파라미터:\n {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 156, 'splitter': 'best'}\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    'max_depth' : [ 6, 8 ,10, 12, 16 ,20, 24]\n}\n\ngrid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 )\ngrid_cv.fit(X_train , y_train)\nprint('GridSearchCV 최고 평균 정확도 수치:{0:.4f}'.format(grid_cv.best_score_))\nprint('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)\n\nFitting 5 folds for each of 7 candidates, totalling 35 fits\nGridSearchCV 최고 평균 정확도 수치:0.8513\nGridSearchCV 최적 하이퍼 파라미터: {'max_depth': 16}\n\n\n\n# GridSearchCV객체의 cv_results_ 속성을 DataFrame으로 생성. \ncv_results_df = pd.DataFrame(grid_cv.cv_results_)\n\n# max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출\ncv_results_df[['param_max_depth', 'mean_test_score']]\n\n\n\n\n\n\n\n\nparam_max_depth\nmean_test_score\n\n\n\n\n0\n6\n0.850791\n\n\n1\n8\n0.851069\n\n\n2\n10\n0.851209\n\n\n3\n12\n0.844135\n\n\n4\n16\n0.851344\n\n\n5\n20\n0.850800\n\n\n6\n24\n0.849440\n\n\n\n\n\n\n\n\nmax_depths = [ 6, 8 ,10, 12, 16 ,20, 24]\n# max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정\nfor depth in max_depths:\n    dt_clf = DecisionTreeClassifier(max_depth=depth, min_samples_split=16, random_state=156)\n    dt_clf.fit(X_train , y_train)\n    pred = dt_clf.predict(X_test)\n    accuracy = accuracy_score(y_test , pred)\n    print('max_depth = {0} 정확도: {1:.4f}'.format(depth , accuracy))\n\n\nparams = {\n    'max_depth' : [ 8 , 12, 16 ,20], \n    'min_samples_split' : [16, 24],\n}\n\ngrid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 )\ngrid_cv.fit(X_train , y_train)\nprint('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))\nprint('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)\n\n\nbest_df_clf = grid_cv.best_estimator_\npred1 = best_df_clf.predict(X_test)\naccuracy = accuracy_score(y_test , pred1)\nprint('결정 트리 예측 정확도:{0:.4f}'.format(accuracy))\n\n\n\nimport seaborn as sns\n\nftr_importances_values = best_df_clf.feature_importances_\n# Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환\nftr_importances = pd.Series(ftr_importances_values, index=X_train.columns  )\n# 중요도값 순으로 Series를 정렬\nftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\nplt.figure(figsize=(8,6))\nplt.title('Feature importances Top 20')\nsns.barplot(x=ftr_top20 , y = ftr_top20.index)\nplt.show()"
  },
  {
    "objectID": "notebooks/ML/5.회귀-checkpoint.html",
    "href": "notebooks/ML/5.회귀-checkpoint.html",
    "title": "5.5 회귀",
    "section": "",
    "text": "회귀의 핵심: 최적의 회귀 계수를 찾는 것.\n분류는 예측값이 이산형 클래스 값이고, 회귀는 연속형 숫자깂이라는 게 가장 큰 차이!\n선형 회귀가 가장 많이 사용됨.\n선형 회귀: 실제 값과 예측 값의 차이를 최소화하는 직선형 회귀선을 최적화하는 방식 -&gt; 규제 방법에 따라 다양한 유형으로 나뉜다.\n선형 회귀 모델의 종류: 일반 선형 회귀, 릿지, 라쏘, 엘라스틱넷, 로지스틱 회귀"
  },
  {
    "objectID": "notebooks/ML/5.회귀-checkpoint.html#다중공선성multi-collinearity-문제",
    "href": "notebooks/ML/5.회귀-checkpoint.html#다중공선성multi-collinearity-문제",
    "title": "5.5 회귀",
    "section": "다중공선성(multi-collinearity) 문제",
    "text": "다중공선성(multi-collinearity) 문제\n: 피처 간의 상관관계가 너무 높아 분산이 매우 커져서 오류에 민감해지는 문제. * 상간관게가 높은 피처가 많은 경우 -&gt; 독립적인 중요한 피처만 남기고 제거하거나 규제를 적용"
  },
  {
    "objectID": "notebooks/ML/5.회귀-checkpoint.html#회귀-평가-지표",
    "href": "notebooks/ML/5.회귀-checkpoint.html#회귀-평가-지표",
    "title": "5.5 회귀",
    "section": "회귀 평가 지표",
    "text": "회귀 평가 지표\n\n1) MAE\n\nMean Absolute Error\n실제 값과 예측값의 차를 절댓값으로 변환해 평균한 것\n\\(MAE = 1/n\\sum_{i=1}^{n} |Yi-Y'i|\\) ### 2) MSE\nMean Squared Error\n실제 값과 예측값 차를 제곱해 평균한 것\n\\(MSE = 1/n\\sum_{i=1}^{n} (Yi-Y'i)^2\\) ### 3) RMSE\nMSE에 루트를 씌운 것 ### 4) \\(R^2\\)\n분산 기반으로 예측 성능 평가\n1에 가까울수록 예측 정확도가 높다.\n\\(R^2 = (예측값 Variance)/(실제값 Variance)\\)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.datasets import load_diabetes\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# boston 데이터 세트 \ndiabetes = load_diabetes()\ndiabetesDF = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ndiabetesDF['sick'] = diabetes.target\nprint(\"데이터 크기\", diabetesDF.shape)\ndiabetesDF.head()\n\n데이터 크기 (442, 11)\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\nsick\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n\n\n\n\n\n\nprint(diabetesDF['sick'].describe())\n\ncount    442.000000\nmean     152.133484\nstd       77.093005\nmin       25.000000\n25%       87.000000\n50%      140.500000\n75%      211.500000\nmax      346.000000\nName: sick, dtype: float64\n\n\n\nfig, axs = plt.subplots(figsize=(16, 8), ncols=5, nrows=2)\nlm_features = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\nfor i, feature in enumerate(lm_features):\n    row = int(i/4)\n    col = i%4\n    sns.regplot(x=feature, y='sick', data=diabetesDF, ax=axs[row][col])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[23], line 6\n      4 row = int(i/4)\n      5 col = i%4\n----&gt; 6 sns.regplot(x=feature, y='sick', data=diabetesDF, ax=axs[row][col])\n\nIndexError: index 2 is out of bounds for axis 0 with size 2\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ny_target = diabetesDF['sick']\nX_data = diabetesDF.drop(['sick'], axis=1, inplace=False)\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.3, random_state=156)\n# 선형 회귀 OLS로 학습/예측/평가 수행\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_preds = lr.predict(X_test)\nmse = mean_squared_error(y_test, y_preds)\nrmse = np.sqrt(mse)\n\nprint('MSE: {0:.3f}, RMSE: {1:.3f}'.format(mse, rmse))\nprint('Variance score: {0:.3f}'.format(r2_score(y_test, y_preds)))\n\nMSE: 2993.705, RMSE: 54.715\nVariance score: 0.497\n\n\n\nprint('절편 값:', lr.intercept_)\nprint('회귀 계수값:', np.round(lr.coef_, 1))\n\n절편 값: 152.38617209733573\n회귀 계수값: [   39.1  -251.9   468.8   305.3 -1146.9   788.    177.2   117.4   937.9\n    53.7]\n\n\n\n# 회귀 계수를 큰 값 순으로 정렬하기 위해 Series로 생성. 인덱스 칼럼명에 유의\ncoeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns)\ncoeff.sort_values(ascending=False)\n\ns5      937.9\ns2      788.0\nbmi     468.8\nbp      305.3\ns3      177.2\ns4      117.4\ns6       53.7\nage      39.1\nsex    -251.9\ns1    -1146.9\ndtype: float64\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\ny_target ="
  },
  {
    "objectID": "notebooks/ML/5.회귀-checkpoint.html#편향-분산-트레이드오프bias-variance-trade-off",
    "href": "notebooks/ML/5.회귀-checkpoint.html#편향-분산-트레이드오프bias-variance-trade-off",
    "title": "5.5 회귀",
    "section": "편향-분산 트레이드오프(Bias-Variance Trade off)",
    "text": "편향-분산 트레이드오프(Bias-Variance Trade off)\n\n편향과 분산은 일반적으로 한쪽이 높으면 한쪽이 낮아지는 경향이 있음.\n‘골디락스’ 지점: 편향을 낮추고 분산을 높이면서 전체 오류가 가장 낮아지는 지점!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "List your projects here. Each project can be a page or a section with links.\n\nProject A: short summary\nProject B: short summary"
  }
]